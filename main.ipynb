{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# visulization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "\n",
    "# test train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# evaluation metrices\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# machine learning\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sarcastic</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>irony</th>\n",
       "      <th>satire</th>\n",
       "      <th>understatement</th>\n",
       "      <th>overstatement</th>\n",
       "      <th>rhetorical_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The only thing I got from college is a caffein...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love it when professors draw a big question ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remember the hundred emails from companies whe...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today my pop-pop told me I was not “forced” to...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VolphanCarol @littlewhitty @mysticalmanatee I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sarcastic  sarcasm  \\\n",
       "0  The only thing I got from college is a caffein...          1      0.0   \n",
       "1  I love it when professors draw a big question ...          1      1.0   \n",
       "2  Remember the hundred emails from companies whe...          1      0.0   \n",
       "3  Today my pop-pop told me I was not “forced” to...          1      1.0   \n",
       "4  @VolphanCarol @littlewhitty @mysticalmanatee I...          1      1.0   \n",
       "\n",
       "   irony  satire  understatement  overstatement  rhetorical_question  \n",
       "0    1.0     0.0             0.0            0.0                  0.0  \n",
       "1    0.0     0.0             0.0            0.0                  0.0  \n",
       "2    1.0     0.0             0.0            0.0                  0.0  \n",
       "3    0.0     0.0             0.0            0.0                  0.0  \n",
       "4    0.0     0.0             0.0            0.0                  0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../Dataset/Sarcasm Dataset.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sarcastic\n",
       "0    2601\n",
       "1     867\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sarcastic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sarcastic_counts = df['sarcastic'].value_counts().reset_index()\n",
    "sarcastic_counts.columns = ['Sarcastic', 'Count']\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.barplot(x='Sarcastic', y='Count', data=sarcastic_counts, hue='Sarcastic', palette='viridis')\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Sarcastic/Not Sarcastic')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['tweet'], inplace=True)\n",
    "df.dropna(subset=['sarcastic'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, X_test, y_test):\n",
    "    # Make predictions on the test set\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average='weighted')\n",
    "    f1_score = metrics.f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Binary Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of words based on raw counts** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3467x10454 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 56597 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sarcastic'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7031700288184438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6988472622478387\n",
      "Random Forest Accuracy: 0.7118155619596542\n",
      "SVM Accuracy: 0.7132564841498559\n",
      "Perceptron Accuracy: 0.6426512968299711\n"
     ]
    }
   ],
   "source": [
    "#### Naive Bayes\n",
    "nb_classifier= MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy)\n",
    "\n",
    "#### Logistic Regression\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "#### Random Forest\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "\n",
    "#### Support Vector Machine (SVM)\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "#### Perceptron\n",
    "perceptron_classifier = Perceptron()\n",
    "perceptron_classifier.fit(X_train, y_train)\n",
    "y_pred_perceptron = perceptron_classifier.predict(X_test)\n",
    "accuracy_perceptron = accuracy_score(y_test, y_pred_perceptron)\n",
    "print(\"Perceptron Accuracy:\", accuracy_perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>0.703170</td>\n",
       "      <td>0.652219</td>\n",
       "      <td>0.703170</td>\n",
       "      <td>0.651175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.698847</td>\n",
       "      <td>0.642973</td>\n",
       "      <td>0.698847</td>\n",
       "      <td>0.643996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.711816</td>\n",
       "      <td>0.624018</td>\n",
       "      <td>0.711816</td>\n",
       "      <td>0.598396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.593881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.642651</td>\n",
       "      <td>0.604197</td>\n",
       "      <td>0.642651</td>\n",
       "      <td>0.618299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Accuracy  Precision    Recall  F1 Score\n",
       "0          Naïve Bayes  0.703170   0.652219  0.703170  0.651175\n",
       "1  Logistic Regression  0.698847   0.642973  0.698847  0.643996\n",
       "2        Random Forest  0.711816   0.624018  0.711816  0.598396\n",
       "3                  SVM  0.713256   0.508735  0.713256  0.593881\n",
       "4           Perceptron  0.642651   0.604197  0.642651  0.618299"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate classifiers\n",
    "nb_results = evaluate_classifier(nb_classifier, X_test, y_test)\n",
    "lr_results = evaluate_classifier(lr_classifier, X_test, y_test)\n",
    "rf_results = evaluate_classifier(rf_classifier, X_test, y_test)\n",
    "svm_results = evaluate_classifier(svm_classifier, X_test, y_test)\n",
    "perceptron_results = evaluate_classifier(perceptron_classifier, X_test, y_test)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Classifier': ['Naïve Bayes', 'Logistic Regression', 'Random Forest', 'SVM', 'Perceptron'],\n",
    "    'Accuracy': [nb_results[0], lr_results[0], rf_results[0], svm_results[0], perceptron_results[0]],\n",
    "    'Precision': [nb_results[1], lr_results[1], rf_results[1], svm_results[1], perceptron_results[1]],\n",
    "    'Recall': [nb_results[2], lr_results[2], rf_results[2], svm_results[2], perceptron_results[2]],\n",
    "    'F1 Score': [nb_results[3], lr_results[3], rf_results[3], svm_results[3], perceptron_results[3]]\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag of words based on TfIDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3467x10454 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 56597 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to array\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "# convert to tf-idf\n",
    "transformer = TfidfTransformer(smooth_idf=False)\n",
    "tfidf_matrix = transformer.fit_transform(X)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, df['sarcastic'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7132564841498559\n",
      "Logistic Regression Accuracy: 0.7161383285302594\n",
      "Random Forest Accuracy: 0.7146974063400576\n",
      "SVM Accuracy: 0.7132564841498559\n",
      "Perceptron Accuracy: 0.6585014409221902\n"
     ]
    }
   ],
   "source": [
    "#### Naive Bayes\n",
    "nb_classifier= MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy)\n",
    "\n",
    "#### Logistic Regression\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "#### Random Forest\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "\n",
    "#### Support Vector Machine (SVM)\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "#### Perceptron\n",
    "perceptron_classifier = Perceptron()\n",
    "perceptron_classifier.fit(X_train, y_train)\n",
    "y_pred_perceptron = perceptron_classifier.predict(X_test)\n",
    "accuracy_perceptron = accuracy_score(y_test, y_pred_perceptron)\n",
    "print(\"Perceptron Accuracy:\", accuracy_perceptron)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.593881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.716138</td>\n",
       "      <td>0.796949</td>\n",
       "      <td>0.716138</td>\n",
       "      <td>0.600587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.714697</td>\n",
       "      <td>0.701074</td>\n",
       "      <td>0.714697</td>\n",
       "      <td>0.599858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.593881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.658501</td>\n",
       "      <td>0.614895</td>\n",
       "      <td>0.658501</td>\n",
       "      <td>0.628834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Accuracy  Precision    Recall  F1 Score\n",
       "0          Naïve Bayes  0.713256   0.508735  0.713256  0.593881\n",
       "1  Logistic Regression  0.716138   0.796949  0.716138  0.600587\n",
       "2        Random Forest  0.714697   0.701074  0.714697  0.599858\n",
       "3                  SVM  0.713256   0.508735  0.713256  0.593881\n",
       "4           Perceptron  0.658501   0.614895  0.658501  0.628834"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate classifiers\n",
    "nb_results = evaluate_classifier(nb_classifier, X_test, y_test)\n",
    "lr_results = evaluate_classifier(lr_classifier, X_test, y_test)\n",
    "rf_results = evaluate_classifier(rf_classifier, X_test, y_test)\n",
    "svm_results = evaluate_classifier(svm_classifier, X_test, y_test)\n",
    "perceptron_results = evaluate_classifier(perceptron_classifier, X_test, y_test)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Classifier': ['Naïve Bayes', 'Logistic Regression', 'Random Forest', 'SVM', 'Perceptron'],\n",
    "    'Accuracy': [nb_results[0], lr_results[0], rf_results[0], svm_results[0], perceptron_results[0]],\n",
    "    'Precision': [nb_results[1], lr_results[1], rf_results[1], svm_results[1], perceptron_results[1]],\n",
    "    'Recall': [nb_results[2], lr_results[2], rf_results[2], svm_results[2], perceptron_results[2]],\n",
    "    'F1 Score': [nb_results[3], lr_results[3], rf_results[3], svm_results[3], perceptron_results[3]]\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **N-grams (bigrams, trigrams)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['sarcastic'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to convert text data into n-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 3))\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test= vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7089337175792507\n",
      "Logistic Regression Accuracy: 0.7132564841498559\n",
      "Random Forest Accuracy: 0.7190201729106628\n",
      "SVM Accuracy: 0.7132564841498559\n",
      "Perceptron Accuracy: 0.6642651296829971\n"
     ]
    }
   ],
   "source": [
    "#### Naive Bayes\n",
    "nb_classifier= MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Naive Bayes Accuracy:\", accuracy)\n",
    "\n",
    "#### Logistic Regression\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_lr)\n",
    "\n",
    "#### Random Forest\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "\n",
    "#### Support Vector Machine (SVM)\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "\n",
    "#### Perceptron\n",
    "perceptron_classifier = Perceptron()\n",
    "perceptron_classifier.fit(X_train, y_train)\n",
    "y_pred_perceptron = perceptron_classifier.predict(X_test)\n",
    "accuracy_perceptron = accuracy_score(y_test, y_pred_perceptron)\n",
    "print(\"Perceptron Accuracy:\", accuracy_perceptron)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naïve Bayes</td>\n",
       "      <td>0.708934</td>\n",
       "      <td>0.649528</td>\n",
       "      <td>0.708934</td>\n",
       "      <td>0.633154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.593881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.798428</td>\n",
       "      <td>0.719020</td>\n",
       "      <td>0.607185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.508735</td>\n",
       "      <td>0.713256</td>\n",
       "      <td>0.593881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.664265</td>\n",
       "      <td>0.603870</td>\n",
       "      <td>0.664265</td>\n",
       "      <td>0.620205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Classifier  Accuracy  Precision    Recall  F1 Score\n",
       "0          Naïve Bayes  0.708934   0.649528  0.708934  0.633154\n",
       "1  Logistic Regression  0.713256   0.508735  0.713256  0.593881\n",
       "2        Random Forest  0.719020   0.798428  0.719020  0.607185\n",
       "3                  SVM  0.713256   0.508735  0.713256  0.593881\n",
       "4           Perceptron  0.664265   0.603870  0.664265  0.620205"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate classifiers\n",
    "nb_results = evaluate_classifier(nb_classifier, X_test, y_test)\n",
    "lr_results = evaluate_classifier(lr_classifier, X_test, y_test)\n",
    "rf_results = evaluate_classifier(rf_classifier, X_test, y_test)\n",
    "svm_results = evaluate_classifier(svm_classifier, X_test, y_test)\n",
    "perceptron_results = evaluate_classifier(perceptron_classifier, X_test, y_test)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Classifier': ['Naïve Bayes', 'Logistic Regression', 'Random Forest', 'SVM', 'Perceptron'],\n",
    "    'Accuracy': [nb_results[0], lr_results[0], rf_results[0], svm_results[0], perceptron_results[0]],\n",
    "    'Precision': [nb_results[1], lr_results[1], rf_results[1], svm_results[1], perceptron_results[1]],\n",
    "    'Recall': [nb_results[2], lr_results[2], rf_results[2], svm_results[2], perceptron_results[2]],\n",
    "    'F1 Score': [nb_results[3], lr_results[3], rf_results[3], svm_results[3], perceptron_results[3]]\n",
    "})\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi Class Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "import pandas as pd\n",
    "\n",
    "class SarcasmDataset:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.df = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train_binary = None\n",
    "        self.y_test_binary = None\n",
    "        self.y_train_multiclass = None\n",
    "        self.y_test_multiclass = None\n",
    "        self.tokenizer = None\n",
    "        self.max_len = None\n",
    "        self.vocab_size = None\n",
    "        self.le_binary = None\n",
    "        self.le_multiclass = None\n",
    "\n",
    "    def load_data(self):\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "        self.df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "        self.df.dropna(subset=['tweet', 'sarcastic'], inplace=True)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        X = self.df['tweet']\n",
    "        y_binary = self.df['sarcastic']\n",
    "        y_multiclass = self.df[['sarcasm', 'irony', 'satire', 'understatement', 'overstatement', 'rhetorical_question']].idxmax(axis=1)\n",
    "\n",
    "        self.le_binary = LabelEncoder()\n",
    "        y_binary = self.le_binary.fit_transform(y_binary)\n",
    "\n",
    "        self.le_multiclass = LabelEncoder()\n",
    "        y_multiclass = self.le_multiclass.fit_transform(y_multiclass)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train_binary, self.y_test_binary = train_test_split(X, y_binary, test_size=0.25, random_state=42)\n",
    "        self.X_train, self.X_test, self.y_train_multiclass, self.y_test_multiclass = train_test_split(X, y_multiclass, test_size=0.25, random_state=42)\n",
    "\n",
    "    def tokenize_data(self):\n",
    "        self.tokenizer = Tokenizer()\n",
    "        self.tokenizer.fit_on_texts(self.X_train)\n",
    "        X_train_seq = self.tokenizer.texts_to_sequences(self.X_train)\n",
    "        X_test_seq = self.tokenizer.texts_to_sequences(self.X_test)\n",
    "\n",
    "        self.max_len = max(len(seq) for seq in X_train_seq)\n",
    "        self.X_train = pad_sequences(X_train_seq, maxlen=self.max_len)\n",
    "        self.X_test = pad_sequences(X_test_seq, maxlen=self.max_len)\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "\n",
    "    def get_data(self):\n",
    "        return (self.X_train, self.X_test, self.y_train_binary, self.y_test_binary,\n",
    "                self.y_train_multiclass, self.y_test_multiclass, self.max_len, self.vocab_size,\n",
    "                self.le_binary.classes_, self.le_multiclass.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = SarcasmDataset('../Dataset/Sarcasm Dataset.csv')\n",
    "dataset.load_data()\n",
    "dataset.preprocess_data()\n",
    "dataset.tokenize_data()\n",
    "\n",
    "# Get the preprocessed data\n",
    "X_train, X_test, y_train_binary, y_test_binary, y_train_multiclass, y_test_multiclass, max_len, vocab_size, binary_classes, multiclass_classes = dataset.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 2 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "Training with 2 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step\n",
      "Training with 3 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "Training with 3 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_layers  dropout_rate  accuracy  precision  recall  f1_score\n",
       "0           2           0.3      0.72       0.51    0.72       0.6\n",
       "1           2           0.7      0.72       0.51    0.72       0.6\n",
       "2           3           0.3      0.72       0.51    0.72       0.6\n",
       "3           3           0.7      0.72       0.51    0.72       0.6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "hyperparameters = [\n",
    "    {'num_layers': 2, 'dropout_rate': 0.3},\n",
    "    {'num_layers': 2, 'dropout_rate': 0.7},\n",
    "    {'num_layers': 3, 'dropout_rate': 0.3},\n",
    "    {'num_layers': 3, 'dropout_rate': 0.7}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in hyperparameters:\n",
    "    num_layers = params['num_layers']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "\n",
    "    # Using preprocessed data from the get_data() method\n",
    "    X_train, X_test, _, _, y_train, y_test, _, _, _, _ = dataset.get_data()\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.LSTM(128, return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.LSTM(128))\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softmax'))  # softmax for multiclass\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f\"Training with {num_layers} layers and dropout rate {dropout_rate}\")\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    results.append({\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'accuracy': round(accuracy, 2),\n",
    "        'precision': round(precision, 2),\n",
    "        'recall': round(recall, 2),\n",
    "        'f1_score': round(f1, 2)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RNN with 2 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n",
      "Training RNN with 2 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Training RNN with 3 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step\n",
      "Training RNN with 3 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_layers  dropout_rate  accuracy  precision  recall  f1_score\n",
       "0           2           0.3      0.63       0.56    0.63      0.59\n",
       "1           2           0.7      0.72       0.51    0.72      0.60\n",
       "2           3           0.3      0.72       0.51    0.72      0.60\n",
       "3           3           0.7      0.72       0.51    0.72      0.60"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters = [\n",
    "    {'num_layers': 2, 'dropout_rate': 0.3},\n",
    "    {'num_layers': 2, 'dropout_rate': 0.7},\n",
    "    {'num_layers': 3, 'dropout_rate': 0.3},\n",
    "    {'num_layers': 3, 'dropout_rate': 0.7}\n",
    "]\n",
    "\n",
    "results_rnn = []\n",
    "\n",
    "for params in hyperparameters:\n",
    "    num_layers = params['num_layers']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.SimpleRNN(128, return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.SimpleRNN(128))\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f\"Training RNN with {num_layers} layers and dropout rate {dropout_rate}\")\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    results_rnn.append({\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'accuracy': round(accuracy, 2),\n",
    "        'precision': round(precision, 2),\n",
    "        'recall': round(recall, 2),\n",
    "        'f1_score': round(f1, 2)\n",
    "    })\n",
    "\n",
    "results_rnn_df = pd.DataFrame(results_rnn)\n",
    "(results_rnn_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU with 2 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 45ms/step\n",
      "Training GRU with 2 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "Training GRU with 3 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step\n",
      "Training GRU with 3 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step\n",
      "   num_layers  dropout_rate  accuracy  precision  recall  f1_score\n",
      "0           2           0.3      0.65       0.60    0.65      0.62\n",
      "1           2           0.7      0.60       0.59    0.60      0.60\n",
      "2           3           0.3      0.61       0.57    0.61      0.59\n",
      "3           3           0.7      0.62       0.58    0.62      0.60\n"
     ]
    }
   ],
   "source": [
    "results_gru = []\n",
    "\n",
    "for params in hyperparameters:\n",
    "    num_layers = params['num_layers']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.GRU(128, return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.GRU(128))\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f\"Training GRU with {num_layers} layers and dropout rate {dropout_rate}\")\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    results_gru.append({\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'accuracy': round(accuracy, 2),\n",
    "        'precision': round(precision, 2),\n",
    "        'recall': round(recall, 2),\n",
    "        'f1_score': round(f1, 2)\n",
    "    })\n",
    "\n",
    "results_gru_df = pd.DataFrame(results_gru)\n",
    "print(results_gru_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BiLSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BiLSTM with 2 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 106ms/step\n",
      "Training BiLSTM with 2 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 81ms/step\n",
      "Training BiLSTM with 3 layers and dropout rate 0.3\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 107ms/step\n",
      "Training BiLSTM with 3 layers and dropout rate 0.7\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step\n",
      "   num_layers  dropout_rate  accuracy  precision  recall  f1_score\n",
      "0           2           0.3      0.63       0.60    0.63      0.61\n",
      "1           2           0.7      0.62       0.58    0.62      0.60\n",
      "2           3           0.3      0.64       0.60    0.64      0.62\n",
      "3           3           0.7      0.66       0.61    0.66      0.63\n"
     ]
    }
   ],
   "source": [
    "results_bilstm = []\n",
    "\n",
    "for params in hyperparameters:\n",
    "    num_layers = params['num_layers']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=100))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True)))\n",
    "        model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)))\n",
    "    model.add(tf.keras.layers.Dense(6, activation='softmax'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(f\"Training BiLSTM with {num_layers} layers and dropout rate {dropout_rate}\")\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "\n",
    "    results_bilstm.append({\n",
    "        'num_layers': num_layers,\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'accuracy': round(accuracy, 2),\n",
    "        'precision': round(precision, 2),\n",
    "        'recall': round(recall, 2),\n",
    "        'f1_score': round(f1, 2)\n",
    "    })\n",
    "\n",
    "results_bilstm_df = pd.DataFrame(results_bilstm)\n",
    "print(results_bilstm_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
